{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kontekstualizovano ugnježdavanje reči"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uvod\n",
    "\n",
    "**Kontekstualizovano ugnježdavanje reči (Contextualized Word Embeddings, CWE)** je postupak formiranja novih reprezentacija reči u zavisnosti od konteksta u kome se reči nalaze. Ideja je pridružiti rečima vektorske reprezantcija takve da je uzet u obzir i kontekst. Na taj način se obuhvata upotreba reči u različitim kotekstima i to znanje ugrađuje u vektorsku reprezentaciju. Formalno, reprezentacije reči odnosno tokena su funkcije od ulazne rečenice. Na primer, reč \"Vašington\" u rečenici \"Univerzitet u Vašingtonu\" treba biti protunačena kao naziv ustanove, jer se u tom kontekstu i koristi, a ne kao lično ime ili naziv grada.\n",
    "\n",
    "Pre razvoja CWE tehnika uglavnom se pristupalo pridruživanju jednog globalnog značenja reči među ostalim rečima u velikim korpusima teksta pomoću modela nenadgledanog učenja:\n",
    "- Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. \"Word representations: a simple and general method for semi-supervised learning\", Koferencija o empirijskim metodama i obradi prirodnih jezika 2014.\n",
    "- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. \"Efficient estimation of word representations in vector space\"\n",
    "- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. \"Glove: Global vectors for word representation\", Koferencija o empirijskim metodama i obradi prirodnih jezika 2014.\n",
    "\n",
    "Ovi radovi nisu uzimali u obzir kontekst u kome se reč nalazi. Jedan od prvih radova koji je uveo kontekst u reporezentacije reči bio je \"Semi-supervised sequence tagging with bidirectional language models\", Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power 2017. Smatra se pretečom modernih CWE modela, koji danas važe za poslednju reč tehike (state-of-the-art). Među njima su radovi:\n",
    "- Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. \"Deep contextualized word repre-sentations\", poznatiji kao ELMO model\n",
    "- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. \"Bert: Pre-training of deep bidirectional transformers for language understanding\"\n",
    "- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. \"Xlnet: Generalized autoregressive pretraining for language understanding\"\n",
    "- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. \"Exploring the limits of transfer learning with a unified text-to-text transformer\"\n",
    "\n",
    "Modeli predstavljni u ovim radovima pokazali su da dobijene CWE reprezentacije reči postižu izuzetne performanse na različitim zadacima obrade prirodnih jezika kao što su klasifikacija teksta, odgovaranje na pitanja, rezimiranje teksta."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sadržaj projekta\n",
    "- U uvodnoj (ovoj) svesci biće predstavljen *\"Contextual String Embeddings for Sequence Labeling\"* Alan Akbik, Duncan Blythe i Roland Vollgraf 2020. i CWE model *\"Contextual string embeddings\"* predstavljen u radu. Naučnici su sve svoje modele i kod organizovali u *[Flair](https://github.com/flairNLP/flair)* radni okvir koji je javno dostupan. Biće predstavljen3 i neke od osnovnih funkcionalnosti Flair radnog okvira.\n",
    "\n",
    "- U drugoj svesci biće predstavljen način za učitavanje skupova podataka koje Flair nudi kao i opšti postupak treniranje modela za prepoznavanje *upos* etiketa uz korišćenje CWE modela iz radnog okvira. Model je treniran na skupu rečenica na srpskom jeziku i rezltati su prikazani na kraju sveske.\n",
    "\n",
    "- U trećoj svesci predstavljen je postupak formiranja CWE modela nad proizvoljnim korpuson. Korišćeni su tekstovi na srpsom jeziku. Model je dalje iskorišćem za treniranje modela za prepoznavanje *upos* etiketa i rezultati su prikazani na kraju sveske"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### \"Contextual String Embeddings for Sequence Labeling\"* Alan Akbik, Duncan Blythe i Roland Vollgraf 2020."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}