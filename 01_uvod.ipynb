{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Kontekstualizovano ugnježdavanje reči"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Uvod\n",
    "\n",
    "**Kontekstualizovano ugnježdavanje reči (Contextualized Word Embeddings, CWE)** je postupak formiranja novih reprezentacija reči u zavisnosti od konteksta u kome se reči nalaze. Ideja je pridružiti rečima vektorske reprezantcija takve da je uzet u obzir i kontekst. Na taj način se obuhvata upotreba reči u različitim kotekstima i to znanje ugrađuje u vektorsku reprezentaciju. Formalno, reprezentacije reči odnosno tokena su funkcije od ulazne rečenice. Na primer, reč \"Vašington\" u rečenici \"Univerzitet u Vašingtonu\" treba biti protunačena kao naziv ustanove, jer se u tom kontekstu i koristi, a ne kao lično ime ili naziv grada.\n",
    "\n",
    "Pre razvoja CWE tehnika uglavnom se pristupalo pridruživanju jednog globalnog značenja reči među ostalim rečima u velikim korpusima teksta pomoću modela nenadgledanog učenja:\n",
    "- Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. \"Word representations: a simple and general method for semi-supervised learning\", Koferencija o empirijskim metodama i obradi prirodnih jezika 2014.\n",
    "- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. \"Efficient estimation of word representations in vector space\"\n",
    "- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. \"Glove: Global vectors for word representation\", Koferencija o empirijskim metodama i obradi prirodnih jezika 2014.\n",
    "\n",
    "Ovi radovi nisu uzimali u obzir kontekst u kome se reč nalazi. Jedan od prvih radova koji je uveo kontekst u reporezentacije reči bio je \"Semi-supervised sequence tagging with bidirectional language models\", Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power 2017. Smatra se pretečom modernih CWE modela, koji danas važe za poslednju reč tehike (state-of-the-art). Među njima su radovi:\n",
    "- Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. \"Deep contextualized word repre-sentations\", poznatiji kao ELMO model\n",
    "- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. \"Bert: Pre-training of deep bidirectional transformers for language understanding\"\n",
    "- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. \"Xlnet: Generalized autoregressive pretraining for language understanding\"\n",
    "- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. \"Exploring the limits of transfer learning with a unified text-to-text transformer\"\n",
    "\n",
    "Modeli predstavljni u ovim radovima pokazali su da dobijene CWE reprezentacije reči postižu izuzetne performanse na različitim zadacima obrade prirodnih jezika kao što su klasifikacija teksta, odgovaranje na pitanja, rezimiranje teksta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sadržaj projekta\n",
    "- U uvodnoj (ovoj) svesci biće predstavljen \"*Contextual String Embeddings for Sequence Labeling*\" Alan Akbik, Duncan Blythe i Roland Vollgraf 2020. i CWE reprezentacije po nazivom \"*Contextual string embeddings*\" predstavljen u radu. Nučnici su sve svoje modele i kod organizovali u *[Flair](https://github.com/flairNLP/flair)* radni okvir koji je javno dostupan. Biće predstavljen3 i neke od osnovnih funkcionalnosti Flair radnog okvira.\n",
    "\n",
    "- U drugoj svesci biće predstavljen način za učitavanje skupova podataka koje Flair nudi kao i opšti postupak treniranje modela za prepoznavanje *upos* etiketa uz korišćenje CWE modela iz radnog okvira. Model je treniran na skupu rečenica na srpskom jeziku i rezltati su prikazani na kraju sveske.\n",
    "\n",
    "- U trećoj svesci predstavljen je postupak formiranja CWE modela nad proizvoljnim korupuson. Korišćeni su tekstovi na srpsom jeziku. Model je dalje iskorišćem za treniranje modela za prepoznavanje *upos* etiketa i rezultati su prikazani na kraju sveske"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### \"Contextual String Embeddings for Sequence Labeling\" Alan Akbik, Duncan Blythe i Roland Vollgraf 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Autori su u radu predstavili model za formiranje CWE reprezentacija reči. Reprezentacije koje model daje su nazvali \"*Contextual string embeddings*\". Glavna karakteristika ovog jezičkog model je to što je on formiran na nivou pojedinačnih karaktera u rečenici. To znači da se za kotekst jedne reči uzima u obzir kompletna sekvenca karaktera ispred i iza nje. Ova karakteristika izdvaja model od ostalih koji pri formiranju konteksta u kome se reč nalazi uzimaju u obzi samo okolne reči a ne cele sekvence karaktera.\n",
    "\n",
    "#### Arhitektura modela\n",
    "\n",
    "Za formiranje \"Contextual string embeddings\" reprezentacija autori su iskoristili LSTM varijanu rekurentnih neuronskih mreža za koju navode da se pokazala kao bolji pristup pri formiranju jezičkih modela u donsu na n-grame, pozivajući se na rezultate iz ranijih radova. Razlog tome je činjenica da LSTM neuralne mreže mogu fleksibilno da modeluju dugoročne zavisnosti sa svojim skrivenim stanjem. Kao što je već rečeno, osnovna grdivna jedinica modela je karakter što za posledicu ima to da se rečenica tretira kao sekvenca karaktera prosleđena LSTM-u, a sam LSTM je treniran da u svakoj tački te sekvence predviđa sledeći karakter.\n",
    "\n",
    "Formalno, cilj jezičkog modela zasnovanog na karakterima je da dobro modeluje raspodelu niza karaktera $P(x_{0:T})$, gde je $(x_0, x_1, ..., x_T):=x_{0:T}$. Treniranjem modela uči se uslovna raspodela karaktera $x_t$ pri datom nizu karaktera, odnosno $P(x_t|x_0,...,x_{t-1})$. Tada se raspodela cele rečenice (čitavog niza karaktera) može dekomponovati na prozvod uslovnih raspodela pojedinačnih karaktera pri nizu karaktera koji im prethode, odnosno \n",
    "\n",
    "$$P(x_{0:T}) = \\prod_{t=0}^{T}{P(x_t|x_{0:t-1})}$$\n",
    "\n",
    "U LSTM mreži uslovna verovatnoća $P(x_t|x_{0:t-1})$ se aproksimira funkcijom od izlaza iz neuronse mreže $\\boldsymbol{h_t}$, odnosno \n",
    "\n",
    "$$P(x_t|x_{0:t-1}) \\approx \\prod_{t=0}^{T}{P(x_t|\\boldsymbol{h_t};\\theta})$$\n",
    "\n",
    "$\\boldsymbol{h_t}$ predstavlja čitavu prošlost sekvence karaktera. U LSTM-u se računa rekurzivno, uz dodatnu pomoć memorijske ćelije $c_t$\n",
    "\n",
    "$$\\boldsymbol{h_t}(x_{0:t-1}) = f_h(x_{t-1}, \\boldsymbol{h_{t-1}}, c_{t-1}; \\theta)$$\n",
    "\n",
    "$$c_t(x_{0:t-1}) = f_c(x_{t-1}, \\boldsymbol{h_{t-1}}, c_{t-1}; \\theta)$$ \n",
    "\n",
    "gde $\\theta$ označava parametre modela a $\\boldsymbol{h_{-1}}$ i $c_{-1}$ se mogu inicijalizovati na nulu ili tretirati kao deo parametara $\\theta$.\n",
    "\n",
    "Na izlaz iz mreže $\\boldsymbol{h_t}$ je postavljen potpuno povezn sloj sa *softmax* aktivacionom funkcijom, tako da je verovatnoća svakog karaktera data sa\n",
    "\n",
    "$$P(x_t|\\boldsymbol{h_t};V)=softmax(V\\boldsymbol{h_t} + b)$$\n",
    "\n",
    "gde su težine $V$ i slobodni članovi $b$ deo parametara modela.\n",
    "\n",
    "Zbog karakteristike ovakve rekurentne neuronske mreže, da se informaciju u skrivenim stanjima šalju u oba smera, uz gore opisani model unapred može se dobiti i model unazad (*eng. backward model*, odakle i dolazi $b$ u eksponentu u formulama ispod), odnosno\n",
    "\n",
    "$$P^b(x_t|x_{t+1:T}) \\approx \\prod_{t=0}^{T}{P^b(x_t|\\boldsymbol{h_t^b};\\theta})$$\n",
    "\n",
    "$$\\boldsymbol{h_t^b} = f_h^b(x_{t+1}, \\boldsymbol{h_{t+1}^b}, c_{t+1}^b; \\theta)$$\n",
    "\n",
    "$$c_t^b = f_c^b(x_{t+1}, \\boldsymbol{h_{t+1}}^b, c_{t+1}^b; \\theta)$$\n",
    "\n",
    "Model unapred i model unazad se koriste na sledći način kako bi se formirala \"*Contextual string embeddings*\" reprezentacija za neku reč:\n",
    "- Iz modela unapred uzima se izlaz skrivenog sloja nakon poslednjeg karaktera u željenoj reči. Kako je model unapred treniran da predvidi sledći karakter u sekvenci, skriveni sloj do tog trenutka nosi kontekst od početka rečenive sve do odabrane reči, uključujuću i tu reč (smer unapred).\n",
    "- Slično, iz modela unazad uzima se izlaz skrivenog sloja pre prvog karaktera u željenoj reči. Kako je model unazad treniran da predvidi prethodni karakter u sekvenici, skriveni sloj do tog trenutka nosi kontekst od kraja rečenice sve do odabrane reči, uljučujuči i tu reč (smer unazad).\n",
    "- Ova dva izlaza se zatim nadovežu kako bi se formirao \"*Contextual string embeddings*\" za željenu reč koji će nositi informaciju o samoj reči ali i informaciju o okolnom kontekstu\n",
    "\n",
    "Formalno, za reč čiji su karakteri $t_0, t_1,..., t_n$ \"*Contextual string embeddings*\" se formira kao\n",
    "$$w_i^{charLM} := \\begin{bmatrix} h^f_{t_{i+1}-1} \\\\ h^b_{t_{i}-1} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opisani postupak je ilustrovan na slici ispod\n",
    "![Formiranje CWE](resources/images/cwe_scheme.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovako dobijene \"*Contextual string embeddings*\" vektorske reprezentacije reči se mogu koristit pri rešavanju različitih problema obrade prirodnih jezika. Način korišćenja je ilustrovan na slici ispod\n",
    "\n",
    "![Koriscenje CWE](resources/images/cwe_usage.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Flair* je radni okvir koji su gore pomenuti naučnici razvili. U okviru Flair-a dostupni su svi modeli koji su pomenuti u radu. Modele je moguće učitati već istrenirane i spremne za korišćenje. Moguće je, takođe, trenirati i sopstveni model sa gore pomenutom arhitekturom. Dostupni su i različiti modeli za obradu prodnih jezika npr. modeli za prepoznavanje imenskih entiteta (*eng. named entity recognition, NER*), modeli za klasifikaciju po vrsti reči (*eng.  part-of-speech tagging, PoS*) i drugi. Dostupni su i različiti CWE modeli ali je isto tako moguće trenirati i svoje CWE modele na proizvoljnim korpusima. Radni okvir podržava i automatsko dohvatanje različitih skupova podataka koji su pravljeni za različite namene. Tu su opšti, ručno etiketirani skupovi tekstualnih podataka. Zatim skupovi podataka namenjeni za rešavanje određenih problema obrade prirodnih jezika kao što su pomenuti NER i PoS problemi. Tu su i skupovio sa biomedicinskim podacima koji se koriste za prepoznvanje imenskih entiteta u toj oblasti. Skupovi su dostupni na raličitim jezicima.\n",
    "\n",
    "Izvorni kod Flair radnog okvira dostupan je na [ovoj](https://github.com/flairNLP/flair) adresi. Za kreiranje modela, Flair u pozadini koristi PyTorch.\n",
    "\n",
    "U nastavku će biti predstavljene neke od osnovnih funkcionalnosti koje Flair nudi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rečenice i tokeni\n",
    "Postoje dva centralna objekta radnog okvira. To su `Sentence` i `Token` objekti, apstrakcije rečenica i tokena. Rečenica u sebi sadrži sam tekst rečenice i u suštini predstavlja listu tokena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Danas je suncan dan .\"\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Formira se objekat recenice\n",
    "sentence = Sentence('Danas je suncan dan.')\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[3]: \"dan\"\n",
      "Token[3]: \"dan\"\n"
     ]
    }
   ],
   "source": [
    "# Dohvatanje tokena\n",
    "print(sentence.get_token(4))\n",
    "print(sentence[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"Danas\"\n",
      "Token[1]: \"je\"\n",
      "Token[2]: \"suncan\"\n",
      "Token[3]: \"dan\"\n",
      "Token[4]: \".\"\n"
     ]
    }
   ],
   "source": [
    "# Iteriranje kroz tokene\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kao što se može videti iz primera koda iznad, kada se formira objekat `Sentence` automatski se radi i tokenizacija rečenice. To je moguće kontrolisati zadavanjem `use_tokenizer` argumenta. U tom slučaju se rečenica deli na reči samo po belinama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The grass is green.\"\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "untokenized_sentence = Sentence('The grass is green.', use_tokenizer=False)\n",
    "print(untokenized_sentence)\n",
    "print(len(untokenized_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moguće koristiti i druge tokenizatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"私 は ベルリン が 好き\"\n"
     ]
    }
   ],
   "source": [
    "from flair.tokenization import JapaneseTokenizer\n",
    "\n",
    "tokenizer = JapaneseTokenizer(\"janome\")\n",
    "\n",
    "japanese_sentence = Sentence(\"私はベルリンが好き\", use_tokenizer=tokenizer)\n",
    "\n",
    "print(japanese_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A moguće je i incijalno podeliti rečenicu na tokene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Danas je suncan dan .\"\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence(['Danas', 'je', 'suncan', 'dan', '.'])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ono što Flair takođe nudi jeste **etiketiranje** bilo rečenica bilo tokena. Ova mogućnost će biti dotsa korišćena u nastavku projekta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Trava je zelena .\" → [\"zelena\"/color]\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence(\"Trava je zelena.\")\n",
    "\n",
    "# Postavljanje etikete reci u recenici\n",
    "# Postavlja se etiketa koja odgovara ner (\"Named entity recognition\") etiketiranju\n",
    "sentence[2].set_label('ner', 'color')\n",
    "\n",
    "# stampanje recenice sa anotacijom\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"Trava\"\n",
      "Token[1]: \"je\"\n",
      "Token[2]: \"zelena\" → color (1.0)\n",
      "Token[3]: \".\"\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text is: \"zelena\"\n",
      "token.idx is: \"3\"\n",
      "label.value is: \"color\"\n",
      "label.score is: \"1.0\"\n"
     ]
    }
   ],
   "source": [
    "token = sentence[2]\n",
    "\n",
    "# Dohvatanje labele\n",
    "label = token.get_label('ner')\n",
    "\n",
    "# Stampanje polja koje token sadrzi\n",
    "print(f'token.text is: \"{token.text}\"')\n",
    "print(f'token.idx is: \"{token.idx}\"')\n",
    "print(f'label.value is: \"{label.value}\"')\n",
    "print(f'label.score is: \"{label.score}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Francuska je trenutni prvak sveta .\" → sports (1.0); soccer (1.0); Serbian (1.0)\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('Francuska je trenutni prvak sveta.')\n",
    "\n",
    "# Moguce je postaviti etikete i celoj recenici \n",
    "# Ovo znaci da recenica ima dve 'topic' etikete\n",
    "sentence.add_label('topic', 'sports')\n",
    "sentence.add_label('topic', 'soccer')\n",
    "\n",
    "# Postavlja se i etiketa za jezik\n",
    "sentence.add_label('language', 'Serbian')\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Francuska je trenutni prvak sveta .\" → sports (1.0)\n",
      "Sentence: \"Francuska je trenutni prvak sveta .\" → soccer (1.0)\n",
      "Sentence: \"Francuska je trenutni prvak sveta .\" → Serbian (1.0)\n"
     ]
    }
   ],
   "source": [
    "# Iteriranje kroz sve etikete\n",
    "for label in sentence.labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Francuska je trenutni prvak sveta .\" → sports (1.0)\n",
      "Sentence: \"Francuska je trenutni prvak sveta .\" → soccer (1.0)\n"
     ]
    }
   ],
   "source": [
    "# Iteriranje kroz odredjeni skup etiketa\n",
    "for label in sentence.get_labels('topic'):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skupovi podataka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skupove podataka moguće je učitati iz modula *datasets*. Ukoliko skup nije lokalno sačuvaj Flair će povući skup sa interneta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 22:26:20,502 Reading data from C:\\Users\\Goran\\.flair\\datasets\\ud_serbian\n",
      "2022-09-01 22:26:20,502 Train: C:\\Users\\Goran\\.flair\\datasets\\ud_serbian\\sr_set-ud-train.conllu\n",
      "2022-09-01 22:26:20,506 Dev: C:\\Users\\Goran\\.flair\\datasets\\ud_serbian\\sr_set-ud-dev.conllu\n",
      "2022-09-01 22:26:20,506 Test: C:\\Users\\Goran\\.flair\\datasets\\ud_serbian\\sr_set-ud-test.conllu\n"
     ]
    }
   ],
   "source": [
    "import flair.datasets\n",
    "corpus = flair.datasets.UD_SERBIAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skupovi su unapred podeljeni na trening, test i skup za validaciju (dev skup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3328\n",
      "520\n",
      "536\n"
     ]
    }
   ],
   "source": [
    "# Stampa se broj recenica u trening skupu\n",
    "print(len(corpus.train))\n",
    "\n",
    "# Stampa se broj recenica u test skupu\n",
    "print(len(corpus.test))\n",
    "\n",
    "# Stampa se broj recenica u validacionom skupu\n",
    "print(len(corpus.dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Beograd i Priština postigli dogovor o slobodi kretanja\" → [\"Beograd\"/Beograd/PROPN/Npmsn/nsubj/Nom/Masc/Sing, \"i\"/i/CCONJ/Cc/cc, \"Priština\"/Priština/PROPN/Npfsn/conj/Nom/Fem/Sing, \"postigli\"/postići/VERB/Vmp-pm/root/Masc/Plur/Past/Part/Act, \"dogovor\"/dogovor/NOUN/Ncmsan/obj/Inan/Acc/Masc/Sing, \"o\"/o/ADP/Sl/case/Loc, \"slobodi\"/sloboda/NOUN/Ncfsl/nmod/Loc/Fem/Sing, \"kretanja\"/kretanje/NOUN/Ncnsg/nmod/Gen/Neut/Sing]\n"
     ]
    }
   ],
   "source": [
    "# Dohvatanje recenice iz skupa\n",
    "sentence = corpus.test[0]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Beograd i Priština postigli dogovor o slobodi kretanja\" → [\"Beograd\"/Npmsn, \"i\"/Cc, \"Priština\"/Npfsn, \"postigli\"/Vmp-pm, \"dogovor\"/Ncmsan, \"o\"/Sl, \"slobodi\"/Ncfsl, \"kretanja\"/Ncnsg]\n"
     ]
    }
   ],
   "source": [
    "# Stampanje samo PoS oznaka recenice\n",
    "print(sentence.to_tagged_string('pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair nudi i automatsko pravljenje **rečnika** iz učitanog korpusa. Moguće je pravite rečnik za sopstveni korpus što će biti urađeno u nastavku projekta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 22:26:42,664 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1267it [00:00, 12421.39it/s]\u001b[A\n",
      "3328it [00:00, 12295.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 22:26:42,943 Dictionary created for label 'upos' with 18 values: NOUN (seen 18103 times), PUNCT (seen 9351 times), ADJ (seen 8835 times), ADP (seen 7130 times), VERB (seen 6406 times), PROPN (seen 5622 times), AUX (seen 4667 times), DET (seen 2848 times), SCONJ (seen 2713 times), ADV (seen 2543 times), CCONJ (seen 2541 times), PRON (seen 1859 times), NUM (seen 944 times), PART (seen 461 times), X (seen 232 times), INTJ (seen 3 times), SYM (seen 1 times)\n",
      "Dictionary with 18 tags: <unk>, NOUN, PUNCT, ADJ, ADP, VERB, PROPN, AUX, DET, SCONJ, ADV, CCONJ, PRON, NUM, PART, X, INTJ, SYM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pravljenje rečnika etiketa za upos (\"Universal Part-of-Speech\") etiketiranje  \n",
    "upos_dictionary = corpus.make_label_dictionary(label_type='upos')\n",
    "\n",
    "# Stampanje recnika\n",
    "print(upos_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukoliko korisniku nije poznato da li se u skupu nalaze etikete neke vrste, moguće je iskoristiti formiranje rečnika nad korpusom sa prosleđenim bilo kakvim argumentom. Tada se štampaju sve vrste etiketa dostupne u skupu ali se i ispaljuje izuzetak kao posledica zadavanja nepostojećeg tipa oznaka koji se može uhvatiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 22:26:46,091 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3328it [00:00, 30256.80it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 22:26:46,210 ERROR: You specified label_type='abcd' which is not in this dataset!\n",
      "2022-09-01 22:26:46,212 ERROR: The corpus contains the following label types: 'lemma' (in 3328 sentences), 'upos' (in 3328 sentences), 'pos' (in 3328 sentences), 'dependency' (in 3328 sentences), 'number' (in 3328 sentences), 'case' (in 3325 sentences), 'gender' (in 3324 sentences), 'verbform' (in 3272 sentences), 'tense' (in 3259 sentences), 'person' (in 3185 sentences), 'mood' (in 3182 sentences), 'degree' (in 3118 sentences), 'definite' (in 2926 sentences), 'prontype' (in 2354 sentences), 'voice' (in 2240 sentences), 'reflex' (in 1115 sentences), 'animacy' (in 1045 sentences), 'numtype' (in 1017 sentences), 'poss' (in 615 sentences), 'polarity' (in 428 sentences), 'number[psor]' (in 283 sentences), 'gender[psor]' (in 156 sentences), 'foreign' (in 121 sentences)\n",
      "Uhvacena greska\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    corpus.make_label_dictionary(label_type='abcd')\n",
    "except Exception as e:\n",
    "    print(\"Uhvacena greska\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair nudi korisniku učitavanje različitih, već istreniranih, modela. Dostupni su \"*Contextual string embeddings*\" kroz moduo `FlairEmbeddings`, zatim druge vrste ugnježdavanja npr.  \"*GloVe*\" ugnježdavanje koji koristi pristup zasnovan na rečim a ne na karakterima. Taj model je dostupa kroz moduo `WordEmbeddings`. Dostupni su takođe već trenirani modeli za etiketiranje teksta raznim vrstama etiketa kroz moduo `SequenceTagger`. U nastavku će biti demostrirano učitavanje i korišćenje ovih modela. \n",
    "\n",
    "Napomena: Flair ove sve već istrenirane modele dohvata sa interneta ukoliko nisu dostupni lokalno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ucitavamo vec istrenirani GloVe model \n",
    "glove_embedding = WordEmbeddings('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[1]: \"grass\"\n",
      "tensor([-0.8135,  0.9404, -0.2405, -0.1350,  0.0557,  0.3363,  0.0802, -0.1015,\n",
      "        -0.5478, -0.3537,  0.0734,  0.2587,  0.1987, -0.1433,  0.2507,  0.4281,\n",
      "         0.1950,  0.5346,  0.7424,  0.0578, -0.3178,  0.9436,  0.8145, -0.0824,\n",
      "         0.6166,  0.7284, -0.3262, -1.3641,  0.1232,  0.5373, -0.5123,  0.0246,\n",
      "         1.0822, -0.2296,  0.6039,  0.5541, -0.9610,  0.4803,  0.0022,  0.5591,\n",
      "        -0.1637, -0.8468,  0.0741, -0.6216,  0.0260, -0.5162, -0.0525, -0.1418,\n",
      "        -0.0161, -0.4972, -0.5534, -0.4037,  0.5096,  1.0276, -0.0840, -1.1179,\n",
      "         0.3226,  0.4928,  0.9488,  0.2040,  0.5388,  0.8397, -0.0689,  0.3136,\n",
      "         1.0450, -0.2267, -0.0896, -0.6427,  0.6443, -1.1001, -0.0096,  0.2668,\n",
      "        -0.3230, -0.6065,  0.0479, -0.1664,  0.8571,  0.2335,  0.2539,  1.2546,\n",
      "         0.5472, -0.1980, -0.7186,  0.2076, -0.2587, -0.3650,  0.0834,  0.6932,\n",
      "         0.1574,  1.0931,  0.0913, -1.3773, -0.2717,  0.7071,  0.1872, -0.3307,\n",
      "        -0.2836,  0.1030,  1.2228,  0.8374])\n"
     ]
    }
   ],
   "source": [
    "# Koristimo recenicu na engleskom jeziku jer su modeli trenirani nad engleskim korpusom\n",
    "sentence = Sentence('The grass is green.')\n",
    "glove_embedding.embed(sentence)\n",
    "\n",
    "# Stampamo vektorsku reprezentaciju tokena\n",
    "print(sentence[1])\n",
    "print(sentence[1].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ucitavamo vec istrenirana Flair ugnjeydavanja. To su ranije pomenuti \"Contextual string embeddings\"\n",
    "# Ucitana su ugnjezdavanja unapred i unazad u skladu sa teorijskom pricom od ranije\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Učitane modele možemo iskoristiti nad rečenicom da bismo dobili vektorske reprezentacije za svaki od njenih tokena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"The\"\n",
      "tensor([-0.0021,  0.0005,  0.0469,  ..., -0.0004, -0.0393,  0.0106])\n",
      "==========================================\n",
      "Token[1]: \"grass\"\n",
      "tensor([-0.0006,  0.0047,  0.0248,  ..., -0.0004, -0.0236,  0.0117])\n",
      "==========================================\n",
      "Token[2]: \"is\"\n",
      "tensor([ 0.0011, -0.0032,  0.0156,  ..., -0.0061,  0.0112,  0.0100])\n",
      "==========================================\n",
      "Token[3]: \"green\"\n",
      "tensor([-0.0034,  0.0003,  0.0256,  ..., -0.0026, -0.0118,  0.0455])\n",
      "==========================================\n",
      "Token[4]: \".\"\n",
      "tensor([ 0.0008,  0.0002,  0.1262,  ..., -0.0002,  0.0039,  0.0058])\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Koristimo recenicu na engleskom jeziku jer su modeli trenirani nad engleskim korpusom\n",
    "sentence = Sentence('The grass is green.')\n",
    "flair_embedding_forward.embed(sentence)\n",
    "\n",
    "# Stampamo dobijene vektorse reprezentacije tokena\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)\n",
    "    print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugnjždavanja možemo da kombinujemo uz pomoć modula `StackedEmbeddings`. To odgovara nadovezivanju reprezentacija iz skrivenih slojeva LSTM mreže opisnom u delu o [arhitekturi modela](#Arhitektura-modela)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import StackedEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sledećim nadovezivanjem ugnježdavanja unapred i unazan dobijamo baš ono ugnježdavanje opisano u radu. Dakle, sada se pravi \"*Contextual string embeddings*\" model kao na prvoj slici u delu o [arhitekturi modela](#Arhitektura-modela) :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    " # \"Contextual string embeddings\" model\n",
    "stacked_embeddings = StackedEmbeddings([                                        \n",
    "                                        flair_embedding_forward,\n",
    "                                        flair_embedding_backward,\n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"Today\"\n",
      "tensor([-1.3909e-03, -3.4892e-05,  3.9613e-02,  ..., -1.8384e-03,\n",
      "        -1.8453e-01, -1.3610e-01])\n",
      "==========================================\n",
      "Token[1]: \"is\"\n",
      "tensor([-6.1987e-03, -3.0133e-03,  9.7788e-02,  ..., -8.2525e-05,\n",
      "        -1.5145e-02, -3.8790e-02])\n",
      "==========================================\n",
      "Token[2]: \"a\"\n",
      "tensor([ 3.4312e-02,  1.0655e-04,  3.8369e-02,  ...,  2.8889e-05,\n",
      "        -1.5626e-02,  1.6645e-02])\n",
      "==========================================\n",
      "Token[3]: \"sunny\"\n",
      "tensor([-0.0009, -0.0039,  0.0238,  ..., -0.0092, -0.0488,  0.0052])\n",
      "==========================================\n",
      "Token[4]: \"day\"\n",
      "tensor([-0.0005, -0.0004,  0.0262,  ..., -0.0008,  0.0331,  0.0076])\n",
      "==========================================\n",
      "Token[5]: \".\"\n",
      "tensor([ 7.0125e-04,  3.7345e-05,  1.0943e-01,  ...,  4.6394e-04,\n",
      "        -1.7693e-02,  3.2021e-03])\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('Today is a sunny day.')\n",
    "stacked_embeddings.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)\n",
    "    print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Možemo učitati i neki od modela za etiketiranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goran\\anaconda3\\envs\\ml\\lib\\site-packages\\huggingface_hub\\file_download.py:621: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-01 22:32:06,741 loading file C:\\Users\\Goran\\.flair\\models\\upos-english\\3489359470b8c3b3c6419514a5f1e27ee827089d6a6b345b4fc2cb5f29b70589.15e4b80e0db9ddfa092bb2a03d56050575455bb50729e3c68617a4aa2f7025ec\n",
      "2022-09-01 22:32:08,241 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, INTJ, PUNCT, VERB, PRON, NOUN, ADV, DET, ADJ, ADP, NUM, PROPN, CCONJ, PART, AUX, X, SYM, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('upos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"George Washington went to Washington .\" → [\"George\"/PROPN, \"Washington\"/PROPN, \"went\"/VERB, \"to\"/ADP, \"Washington\"/PROPN, \".\"/PUNCT]\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('George Washington went to Washington.')\n",
    "\n",
    "# Previdjanje UPOS etiketa\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaključak\n",
    "\n",
    "Ovim su pokrivene osnove CWE koncepta, osnovne funkcionalnosti Flair radnog okvira kao i teorijske osnove i arhitektura iza modela koje okvir nudi. U nastavku se može očekivati pravljenje rečnika na proizvoljnom korpusu, treniranje \"Contextual string embeddings\" modela na proizvoljnom korpusu, treniranje modela za etiketiranje uz koršćenje vektorskih reprezentacija. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
